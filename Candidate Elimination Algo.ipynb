{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ef2c2034676875dd678d3773e7e92b3",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2023) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Leila Malihi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58f810e20f7e7ebbafecc0c340f8d947",
     "grade": false,
     "grade_id": "cell-175c5b8f652e026d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet 01: Concept Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6333dd1a097178319cc59e5641a3c44c",
     "grade": false,
     "grade_id": "cell-0779efa0bccb27ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This is the first official exercise sheet. The homework sheets will usually be available at the beginning of the week and are supposed to be solved in groups of three. They have to be handed in by the end of Sunday of that week. The exercises are then presented to your tutor in a small feedback session. To acquire the admission for the final exam, you will have to pass $N-2$ of the weekly provided exercise sheets.\n",
    "\n",
    "Sign up for a group on Stud.IP (See `Participants` -> `Functions/Groups`). The times mentioned there are the times for the feedback session of your group. If none of them fits, send any of the tutors an e-mail so we can try to arrange something.\n",
    "\n",
    "Your group will have a group folder in Stud.IP under `Documents`. Upload your solutions there to hand them in.\n",
    "\n",
    "All exercise sheets will use [Jupyter Notebooks](http://jupyter-notebook.readthedocs.org/en/latest/notebook.html). To be able to run these on your system, you will need to install Python and a few packages. We suggest the newest version of Python 3 and installing the conda environment as explained in the practice session and in the file \"ml-install.txt\" (found on Stud.IP in the \"Documents\" section in the Folder \"Exercises\". This was also part of Sheet 00).\n",
    "\n",
    "This week's sheet should be solved and handed in before end of **Sunday, April 23, 2023**. \n",
    "Please upload your results to your group's Stud.IP folder. In case you cannot do this first sheet (due to technical or organizational problems) please upload a description of your problem instead. Your tutor will help you to solve the problems in the first feedback session and you may hand in this sheet together with the second sheet one week later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "277cc8cd5c7024446f24e776834a0248",
     "grade": false,
     "grade_id": "cell-aad21d3e70ca29e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Learning Tasks (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bba3d0602b8325424fd3eab7361a9c",
     "grade": false,
     "grade_id": "cell-573de581339f4602",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(a)** Give three computer applications for which machine learning approaches seem \n",
    "appropriate and three for which they seem inappropriate. Pick applications that are not\n",
    "already mentioned in the lecture, and include a one-sentence justification for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151bf450b5c61e6bceed135c8b01dd4a",
     "grade": true,
     "grade_id": "cell-7daad0c311c13e13",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Applications of Machine learning:\n",
    "1.Face recognition>> Since it helps in tagging in Facebook friends\n",
    "2.handwritten recognition\n",
    "3.credit card approval\n",
    "4.Image recognition\n",
    "5.Sentimental analysis.\n",
    "\n",
    "Non Machine Learning:\n",
    "1.execute a query to database>> Query is different language and can't be handled by Machine learning as external supervision \n",
    "2.comedy>>requires a bizarre ex nihilo and sponteneity (distinguishable from three above?) in fact, the second and third above are inappropriate, rather? define “inappropriate”: difficult? vague performance measure?\n",
    "3.new science and mathematics>>can “creativity” be modelled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12ca17fa7966ec967a3229cf582b1a61",
     "grade": false,
     "grade_id": "cell-75f5683d29ef7800",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(b)** Pick some learning task not mentioned in the lecture. Describe it informally in a paragraph in English. Now describe it by stating as precisely as possible the task, performance measure, and training experience. Finally, propose a target function to be learned and a target representation. Discuss the main tradeoffs you considered in formulating this learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81cdd034de4a4ebb66b3b887fc985a1b",
     "grade": true,
     "grade_id": "cell-ded8a09479800067",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For a system being designed to detect spam emails, TPE would be,\n",
    "\n",
    "Task T: To recognise and classify emails into 'spam' or 'not spam'.\n",
    "\n",
    "Performance measure P: Total per cent of emails being correctly classified as 'spam' (or 'not spam' ) by the program.\n",
    "\n",
    "Training experience E: A set of emails with given labels ('spam' / 'not spam').\n",
    "\n",
    "target function V : progression → S; V (s = Spam)=+ve, V (s = Not spam)=-ve.\n",
    "\n",
    "target function representation Vˆ(s) = w0+w1x1, where x1 = given mails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Prove that the LMS weight update rule described in (ML-01, slide 19) performs a gradient descent to minimize the squared error given on that slide: calculate the derivative of $E$ with respect to the weight $w_i$, assuming that $V(b)$ is a linear function. Gradient descent is achieved by updating each weight in proportion to $-\\partial E/\\partial w_i$. Therefore, you must show that the LMS training rule alters weights in this proportion for each training example it encounters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c137bc4993805b4eaef542e3f5b8813d",
     "grade": true,
     "grade_id": "cell-ed7d79641fd2df77",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Given the target function representation od = w0 + w1x1 + … + wnxn,\n",
    "\n",
    "LMS training rule is a learning algorithm for choosing the set of weights wi to best fit the set of training examples {< d, td >}, i.e., to minimize the squared error\n",
    "\n",
    "E ≡ ∑d∈D (td – od)² --------(*)\n",
    "\n",
    "LMS training rule works as follows: (∀ < d, td >) use the current weights wi to calculate od (∀wi) wi ← wi + η(td – od)xid\n",
    "\n",
    "Now, The gradient decent algorithm: Δwi = –α (∂E / ∂wi)\n",
    "\n",
    "First represent (∂E / ∂wi) in terms of the unit inputs xid, outputs od, and target values td: (∂E / ∂wi) = (∂∑d∈D (td – od)²) / ∂wi\n",
    "\n",
    "       = ∑d∈D 2(td – od) (∂(td – od) / ∂wi) \n",
    "       =∑d∈D 2(td – od) (–∂od / ∂wi) \n",
    "       = –∑d∈D 2(td – od) (∂(w0 + … + wixid + … + wnxnd) /∂wi) \n",
    "       = –∑d∈D 2(td – od) (xid)\n",
    "=> (1/2xid)(∂E / ∂wi) = (td – od)\n",
    "\n",
    "Now, From (*),\n",
    "\n",
    "(∀wi) wi ← wi + (η/2)(–∂E / ∂wi)\n",
    "\n",
    "This shows that LMS alters weights in the very same proportion as does the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bc7512c6e1323866c514415501c7127",
     "grade": false,
     "grade_id": "cell-d0903956896707a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Candidate Elimination by Hand (6 points)\n",
    "\n",
    "Candidate Elimination is a learning algorithm that, in each step, tries to generate a description which is consistent with all previously observed examples in a training set. That description could hypothetically then be used to classify examples outside the training set.\n",
    "\n",
    "Consider the following situation:\n",
    "\n",
    "Earl and Fran have made it their mission to visit as many amusement parks as possible after the pandemic. However, to maximize their enjoyment and not have any unnecessary arguments break out, they make a list of previous park visits and if they would go there again, to have a few criteria to decide if a park is worth their time.\n",
    "\n",
    "This is the set of attributes along with their possible values Earl and Fran came up with:\n",
    "\n",
    "| Attribute           | driving distance | ticket price      | rollercoasters | dinosaurs |\n",
    "|---------------------|------------------|-------------------|----------------|-----------|\n",
    "| **Possible Values** | short / far      | cheap / expensive | many / none    | yes / no  |\n",
    "\n",
    "This is Earl and Fran's accumulated data from previous visits. The list will allow you to come to a learning decision which properties have to be fulfilled such that the two will enjoy a visit to an amusement park.\n",
    "\n",
    "| Sample No. | driving distance | ticket price | rollercoasters | dinosaurs | go again? |\n",
    "|------------|------------------|--------------|----------------|-----------|-----------|\n",
    "| 1          | far              | cheap        | many           | no        | yes       |\n",
    "| 2          | short            | expensive    | many           | no        | yes       |\n",
    "| 3          | far              | expensive    | none           | yes       | no        |\n",
    "| 4          | short            | cheap        | none           | yes       | no        |\n",
    "| 5          | short            | cheap        | many           | yes       | yes       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b4703448c8b663770de21b564adc7d3",
     "grade": false,
     "grade_id": "cell-f5cd6d758c895950",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Apply Candidate Elimination to the samples 1-5 below and provide the version space boundaries $S_n$ and $G_n$ after each new training sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fec90624fcecf2c434a846108010f7d4",
     "grade": true,
     "grade_id": "cell-ab417b404920afe1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "G1: [('?', '?', '?', '?')]\n",
    "S1: [('far', 'cheap', 'many', 'no')]\n",
    "\n",
    "\n",
    "G2: [('?', '?', '?', '?')]\n",
    "S2: [('?', '?', 'many', 'no')]\n",
    "\n",
    "\n",
    "G3: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
    "S3: [('?', '?', 'many', 'no')]\n",
    "\n",
    "\n",
    "G4: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
    "S4: [('?', '?', 'many', 'no')]\n",
    "\n",
    "\n",
    "G5: [('?', '?', 'many', '?')]\n",
    "S5: [('?', '?', 'many', '?')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cd47301fa3634fba84344fd78bfdc05",
     "grade": false,
     "grade_id": "cell-e5db1c57702dcd02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Provide the complete version space bounded by $S_2$ and $G_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2f3537c4bfd9cd00261e1f5895de7ab",
     "grade": true,
     "grade_id": "cell-ef42d1e895e9cff6",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "G2: [('?', '?', '?', '?')] S2:[('?', '?', 'many', 'no')]\n",
    "Version space:\n",
    "[('far', 'cheap', 'many', 'no')],[('short', 'cheap', 'many', 'no')],[('far', 'expensive', 'many', 'no')],[('short', 'expensive', 'many', 'no')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "456f728ea77936ea39878de7fec2f127",
     "grade": false,
     "grade_id": "cell-2375670feb946ca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** To what kind of amusement park should Earl and Fran go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "806471df2ca25eb95db37871a730d9a5",
     "grade": true,
     "grade_id": "cell-8aa8761eb3b1780c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    " Earl and Fran can go to amusement parks having many rollercoasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "743d52595090016b580fb83ea7404793",
     "grade": false,
     "grade_id": "cell-c0e07cdee1f5cee5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Candidate Elimination in Python (6 points)\n",
    "\n",
    "Now let's get to real fun part: Programming our first machine learning algorithm!\n",
    "\n",
    "But first some general remarks on coding style:\n",
    "\n",
    "In general, try to write code that's consistent with [PEP8](https://www.python.org/dev/peps/pep-0008/), the offical Python Style Guide. Have a look a the [Google Style Guide](https://google.github.io/styleguide/pyguide.html) as well. It's based on PEP8 and has some nice examples. As a bonus always document your function and classes with docstrings. It is best practice to stick to some docstring convention for example [Google's](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html). This has the big advantages that it allows for automated generation of documentation. We will follow that convention in the code we provide you, so you know what kind of objects to expect and what to return.\n",
    "\n",
    "If, during the programming tasks, you run into a `NameError`, make sure that you executed all prior code cells beforehand. Later cells might rely on variables and function from prior cells. To see all currently defined variables you can make use of the `%whos` [magic function](https://github.com/lmmx/devnotes/wiki/IPython-'magic'-function-documentation#whos) anywhere in code cells. Additionally, it is sometimes handy to run all cells from the beginning by opeining the command palette typing `run all cells`. Moreover, using <kbd>b</kbd> to add new cells below and <kbd>a</kbd> for adding new cells above your current cell will make your life often easier. Finally, using <kbd>l</kbd>  to show line numbers is helpful for locating errors from error messages.\n",
    "\n",
    "In the following Python code we have provided the building blocks for the `CANDIDATE-ELIMINATION` algorithm and put them together in a single function. Now you have to fill those building blocks with actual code. There are places marked with \n",
    "\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "``` \n",
    "\n",
    "where you have to add some code to make the code work. Finish the code to automate the decision making for Earl and Fran!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1be7dc368d4765d7c4ec40f3c795ed2d",
     "grade": true,
     "grade_id": "ex2_solution1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_consistent(h, datum):\n",
    "    \"\"\"\n",
    "    Checks if a general hypothesis is consistent with a datum.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): Hypothesis. '?' indicates all values possible, '0' indicates no value possible.\n",
    "        datum (dict): Datum with 'values' (tuple) and 'target' (bool).\n",
    "        \n",
    "    Returns:\n",
    "        bool: Whether the hypothesis correctly predicts the target value.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    count = 0\n",
    "    if h == tuple(\"?\") * len(datum[\"values\"]) and datum[\"target\"]== True:\n",
    "                             return True\n",
    "    elif h == tuple(\"0\") * len(datum[\"values\"]) and datum[\"target\"]==False:\n",
    "                             return True\n",
    "    else:\n",
    "      for i in range(len(h)):\n",
    "        if h[i] == datum[\"values\"][i] or h[i] == \"?\":\n",
    "            count = count+1\n",
    "      if count == len(h) and datum[\"target\"]==True:\n",
    "        return True\n",
    "      elif count != len(h) and datum[\"target\"]==False:\n",
    "        return True\n",
    "      else:\n",
    "        return False\n",
    "\n",
    "assert is_consistent(('far', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': True })\n",
    "assert not is_consistent(('far', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': False })\n",
    "assert is_consistent(('short', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': False })\n",
    "assert not is_consistent(('short', 'cheap', '?', 'no'), {'values': ('far', 'cheap', 'many', 'no' ), 'target': True })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cabc8f16ae23164b6ad4a001fcb0d7d3",
     "grade": true,
     "grade_id": "ex2_solution2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_more_general_or_equal(h1, h2):\n",
    "    \"\"\"\n",
    "    Checks whether hypothesis h1 is more general than hypothesis h2 or equally general.\n",
    "    \n",
    "    Args:\n",
    "        h1 (tuple): Hypothesis 1.\n",
    "        h2 (tuple): Hypothesis 2.\n",
    "           '?' indicates all values possible, '0' indicates no value possible.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the predicate is satisfied.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    c =0\n",
    "    for i in range(len(h1)):\n",
    "          if h1[i] == h2[i]:\n",
    "            c = c+1\n",
    "          elif h1[i] != h2[i] and h1[i] == \"?\":\n",
    "            c = c+1\n",
    "    if c == len(h1):\n",
    "      return True\n",
    "    \n",
    "assert is_more_general_or_equal(('?', '?', '?', '?'), ('far', 'cheap', 'many', 'no'))\n",
    "assert not is_more_general_or_equal(('?', '?', 'many', 'no'), ('far', 'cheap', 'many', '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a15cddefdcdf95d1b2ca494a693e47",
     "grade": true,
     "grade_id": "ex2_solution3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_more_specific_or_equal(h1, h2):\n",
    "    \"\"\"\n",
    "    Checks whether hypothesis h1 is more specific than hypothesis h2 or equally specific.\n",
    "    \n",
    "    Args:\n",
    "        h1 (tuple): Hypothesis 1.\n",
    "        h2 (tuple): Hypothesis 2.\n",
    "           '?' indicates all values possible, '0' indicates no value possible.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the predicate is satisfied.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    c = 0\n",
    "    for i in range(len(h1)):\n",
    "          if h1[i] == h2[i]:\n",
    "            c = c+1\n",
    "          elif h1[i] != h2[i] and h2[i] == \"?\":\n",
    "            c = c+1\n",
    "    if c == len(h1):\n",
    "      return True\n",
    "    \n",
    "assert is_more_specific_or_equal(('?', '?', 'many', 'no'), ('?', '?', 'many', '?'))\n",
    "assert not is_more_specific_or_equal(('?', 'cheap', 'many', 'no'), ('far', '?', '?', '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b3b1e043cecc2374b3f95707bf3847",
     "grade": true,
     "grade_id": "ex2_solution4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generalize_minimally(h, datum):\n",
    "    \"\"\"Generalize hypothesis h so it becomes consitent with the datum.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): The hypothesis to be generalized.\n",
    "                   '?' indicates all values possible, '0' indicates no value possible.\n",
    "        datum (tuple): Attribute values of a datum. The datum is assumed to have a positive target value.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The generalized hypothesis.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if h == tuple(\"0\") * len(datum):\n",
    "        return datum\n",
    "    else :\n",
    "      h = list(h)\n",
    "      for i in range(len(h)):\n",
    "        \n",
    "        if h[i] != datum[i]:\n",
    "          h[i] = \"?\"\n",
    "\n",
    "      h = tuple(h)\n",
    "      return h\n",
    "\n",
    "assert generalize_minimally(('?', '?', 'many', 'no'), ('short', 'cheap', 'many', 'yes')) == ('?', '?', 'many', '?')\n",
    "assert generalize_minimally(('0', '0', '0', '0'), ('short', 'cheap', 'many', 'yes')) == ('short', 'cheap', 'many', 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25bc1300e17e3677b2333d4a4afcf904",
     "grade": true,
     "grade_id": "ex2_solution5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def specialize_minimally(h, datum, attr_values):\n",
    "    \"\"\"\n",
    "    Generate all consistent minimal specialization of hypothesis h.\n",
    "    \n",
    "    Args:\n",
    "        h (tuple): The hypothesis to be specialized.\n",
    "        datum (tuple): Attribute values of a datum. The datum is assumed to have a negative target value.\n",
    "        attr_values (tuple of tuples): All possible attribute values for each attribute.\n",
    "    \n",
    "    Returns:\n",
    "        tuple of tuples: Tuple of the specialized hypotheses.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tup = []\n",
    "    \n",
    "    if h == tuple(\"?\") * len(datum):\n",
    "          for i in range(len(datum)):\n",
    "            temp = list(h)\n",
    "            opp = list(attr_values[i])\n",
    "            opp.remove(datum[i])\n",
    "            temp[i] = opp[0]\n",
    "            tup.append(tuple(temp))\n",
    "\n",
    "          return tuple(tup)\n",
    "\n",
    "    else:\n",
    "      for i in range(len(h)):\n",
    "              temporary = list(h)\n",
    "    \n",
    "              if h[i] != datum[i]:\n",
    "\n",
    "                  opp = list(attr_values[i])\n",
    "                  opp.remove(datum[i])\n",
    "\n",
    "                  temporary[i] = opp[0]\n",
    "                  tup.append(tuple(temporary))\n",
    "\n",
    "      return tuple(tup)\n",
    "\n",
    "attr_values = (('short', 'far'), ('cheap', 'expensive'), ('many', 'none'), ('yes', 'no'))\n",
    "assert specialize_minimally(('?', '?', 'many', 'no'), ('short', 'cheap', 'many', 'no'), attr_values) == (('far', '?', 'many', 'no'), ('?', 'expensive', 'many', 'no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113fceeb10337e3cec2bd17e71f3a3e2",
     "grade": false,
     "grade_id": "cell-eed96524af5d38e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now check that the algorithm works in the intended way by excecuting the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05fafec19305a3fd93d0444af9a92260",
     "grade": false,
     "grade_id": "cell-5ff73aa56439d69d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def eliminate_candidates(data, attr_values):\n",
    "    \"\"\"\n",
    "    Candidate elimination algorithm printing its progress at each step.\n",
    "    \n",
    "    Args:\n",
    "        data (list of dicts): The dataset.\n",
    "        attr_values (tuple of tuples): All possible attribute values for each attribute in the data.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The general and the specific boundary of the version space.\n",
    "    \"\"\"\n",
    "    # Initialize general and specific boundary.\n",
    "    \n",
    "    # Maximally general hypothesis.\n",
    "    general_boundary = [tuple('?') * len(attr_values)]\n",
    "    # Maximally specific hypothesis.\n",
    "    specific_boundary = [tuple('0') * len(attr_values)]\n",
    "    \n",
    "    # Fit the version space to the data.\n",
    "    for datum in data:\n",
    "        if datum['target']:\n",
    "            # If the sample is classified as positive...\n",
    "            \n",
    "            # Remove all inconsistent hypotheses from G.\n",
    "            general_boundary = [g for g in general_boundary if is_consistent(g, datum)]\n",
    "            \n",
    "            for s in specific_boundary:\n",
    "                # Remove all inconsistent hypothesis from S.\n",
    "                if not is_consistent(s, datum):\n",
    "                    specific_boundary.remove(s)\n",
    "\n",
    "                    # Add to S all minimal generalizations s. In this case only one.\n",
    "                    s_generalized = generalize_minimally(s, datum['values'])\n",
    "                    # Add the new hypothesis to the specific boundary, if it is not more general \n",
    "                    # than the general boundary. We do not need to check for consistency again\n",
    "                    # as the hypothesis was constructed in such a way that it must be consistent.\n",
    "                    if any(is_more_general_or_equal(g, s_generalized) for g in general_boundary):\n",
    "                        specific_boundary.append(s_generalized)\n",
    "\n",
    "            # Remove from S any hypothesis that is more general than another hypothesis in S.\n",
    "            for s in specific_boundary:\n",
    "                if any(is_more_general_or_equal(s, s2) \n",
    "                       and not s == s2 for s2  \n",
    "                       in specific_boundary):\n",
    "                    \n",
    "                    specific_boundary.remove(s)\n",
    "\n",
    "        else:\n",
    "            # If the sample is classified as negative...\n",
    "            \n",
    "            # Remove all inconsistent hypotheses from S.\n",
    "            specific_boundary = [s for s in specific_boundary if is_consistent(s, datum)]\n",
    "            for g in general_boundary:\n",
    "                # Remove all inconsistent hypotheses from G.\n",
    "                if not is_consistent(g, datum):\n",
    "                    general_boundary.remove(g)\n",
    "\n",
    "                    # Add to G all minimal specializations of g.\n",
    "                    for specialized_g in specialize_minimally(g, datum['values'], attr_values):\n",
    "                        # Add the new specialized hypothesis to the general boundary, if it is not more \n",
    "                        # specific than the specific boundary.\n",
    "                        # We do not need to check for consistency again\n",
    "                        # as the hypothesis was constructed in such a way that it must be consistent.\n",
    "                        if any(is_more_specific_or_equal(s, specialized_g) for s in specific_boundary):\n",
    "                                \n",
    "                                general_boundary.append(specialized_g)\n",
    "                \n",
    "                # Remove from G any hypothesis that is less general than another hypothesis in G.\n",
    "                for g in general_boundary:\n",
    "                    if any(is_more_specific_or_equal(g, g2) \n",
    "                           and not g == g2 for g2 \n",
    "                           in general_boundary):\n",
    "                        \n",
    "                        general_boundary.remove(g)\n",
    "        \n",
    "        # Print progress of algorithm at each iteration.\n",
    "        print('Sample: {} {}\\nG: {}\\nS: {}\\n'.format('+' if datum['target'] else '-', datum['values'],\n",
    "                                                     general_boundary, specific_boundary))\n",
    "        \n",
    "    return general_boundary, specific_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0c236e39672214b868e7126f82e45ed",
     "grade": false,
     "grade_id": "ex2_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: + ('far', 'cheap', 'many', 'no')\n",
      "G: [('?', '?', '?', '?')]\n",
      "S: [('far', 'cheap', 'many', 'no')]\n",
      "\n",
      "Sample: + ('short', 'expensive', 'many', 'no')\n",
      "G: [('?', '?', '?', '?')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: - ('far', 'expensive', 'none', 'yes')\n",
      "G: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: - ('short', 'cheap', 'none', 'yes')\n",
      "G: [('?', '?', 'many', '?'), ('?', '?', '?', 'no')]\n",
      "S: [('?', '?', 'many', 'no')]\n",
      "\n",
      "Sample: + ('short', 'cheap', 'many', 'yes')\n",
      "G: [('?', '?', 'many', '?')]\n",
      "S: [('?', '?', 'many', '?')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('?', '?', 'many', '?')], [('?', '?', 'many', '?')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_values = (('short', 'far'), ('cheap', 'expensive'), ('many', 'none'), ('yes', 'no'))\n",
    "\n",
    "# samples\n",
    "data = [ \n",
    "    {'values': ('far',   'cheap',     'many', 'no' ), 'target': True },\n",
    "    {'values': ('short', 'expensive', 'many', 'no' ), 'target': True },\n",
    "    {'values': ('far',   'expensive', 'none', 'yes'), 'target': False},\n",
    "    {'values': ('short', 'cheap',     'none', 'yes'), 'target': False},\n",
    "    {'values': ('short', 'cheap',     'many', 'yes'), 'target': True }\n",
    "]\n",
    "\n",
    "eliminate_candidates(data, attr_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91e4a84f5853ecbc52d783a0e692c56f",
     "grade": false,
     "grade_id": "cell-e8704cc99397489d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Inductive Bias (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd0acfdf4a321604a638f246e6929f77",
     "grade": false,
     "grade_id": "cell-0cac3f19657cf08e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** What is an inductive bias? Describe the concept in your own words! Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2da9a37d81c2fe25f07feb280b1ae8ab",
     "grade": true,
     "grade_id": "cell-ef3d6719a20fcc57",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "It refers to a set of assumptions made by algorithm to generalise set of observations.It is required because it can influence the model’s ability to learn from a given dataset and can affect the performance of the model on new, unseen data.\n",
    "Since a model gets its ability of learning by means of biasing in some ways or other. Hence Inductive bias becomes essential when comes to learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b92b9dc60ffc40224adcdf36cc5242e1",
     "grade": false,
     "grade_id": "cell-d2f09c6e12b491df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Describe and compare the inductive bias for the learning algorithms you heard about in the lecture (Candidate Elimination and Find-S)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a38c78a5ac03052544c415aa7f4f1d1e",
     "grade": true,
     "grade_id": "cell-4ebc56901ad0ad00",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Inductive bias is a collection of implicit or explicit assumptions that machine learning algorithms make in order  to forecast outcomes of given inputs that it has never seen before.\n",
    "Inductive reasoning is the process of learning general principles based on particular cases and hence the Inductive bias favors one set of generalisations over other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e16501608f4498a96416dd4de88836cf",
     "grade": false,
     "grade_id": "cell-c30597e059264a70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** A concept can be considered as a boolean-valued function on the instance space. Asumming a representation using $n$ binary features, how many different concepts exist? Using the concept representation introduced in the lecture (conjuntion of constraints on the attributes), how many of these concepts can be expressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad154877cfa90208410eef922ba8fd9a",
     "grade": true,
     "grade_id": "cell-5b79bd62c4d58ce6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "2^2^n different concepts exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6ba7970da7ed7d0c6854e8917932e26",
     "grade": false,
     "grade_id": "cell-e21f11a5856571ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**d)** The *power* of a hypotheses space is defined as the cardinality of the largest set of inputs that can be labeled by the hypotheses in all possible ways. Consider now a representation with $n=2$ binary features. What is the power of the unrestricted hypotheses space? Using the concept representation from the lecture, what is the power of the restricted hypotheses space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "177178813f001385e51ba4fb3dd56f14",
     "grade": true,
     "grade_id": "cell-281f1d049ce89888",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "power of the unrestricted hypotheses space is 4^n\n",
    "\n",
    "power of Restricted hypotheses space is 3^n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
